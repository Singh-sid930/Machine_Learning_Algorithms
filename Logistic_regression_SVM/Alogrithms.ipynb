{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "'''\n",
    "    TEMPLATE FOR MACHINE LEARNING HOMEWORK\n",
    "    AUTHOR Eric Eaton\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, alpha = 0.01, regLambda= 0.01 , regNorm=2, epsilon=0.000001, maxNumIters = 2000):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "        \talpha is the learning rate\n",
    "        \tregLambda is the regularization parameter\n",
    "        \tregNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "        \tepsilon is the convergence parameter\n",
    "        \tmaxNumIters is the maximum number of iterations to run\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.regLambda = regLambda\n",
    "        self.regNorm = regNorm\n",
    "        self.epsilon = epsilon\n",
    "        self.maxNumIters =  maxNumIters\n",
    "        self.JHist = None\n",
    "        self.theta = None\n",
    "    \n",
    "\n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "        yhat = self.sigmoid(X*theta)\n",
    "        if (self.regNorm == 2): # L-2 Norm\n",
    "            J = -(y.T*np.log(yhat)) - ((1.0-y).T*(np.log(1.0 - yhat))) + (self.regLambda/2.0)* (theta.T*theta )\n",
    "        else:                   # L-1 Norm\n",
    "            J = -(y.T*np.log(yhat)) - ((1.0-y).T*(np.log(1.0 - yhat))) + (self.regLambda/2.0)* (np.sum(theta))\n",
    "        J_scalar = J.item((0,0))\n",
    "        return J_scalar\n",
    "    \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "        yhat = self.sigmoid(X*theta)\n",
    "        gradient = (X.T * (yhat - y))\n",
    "        # Account for regularization in theta\n",
    "         # Account for regularization in theta\n",
    "        if (self.regNorm == 2):\n",
    "            for k in range(1,len(theta)):\n",
    "                gradient[k]= gradient[k] + (regLambda*theta[k])\n",
    "        else:\n",
    "            for k in range(1,len(theta)):\n",
    "                gradient[k]= gradient[k] + (regLambda*np.sign(theta[k]))\n",
    "        gradient[0] = sum(yhat-y)\n",
    "        return gradient\n",
    "\n",
    "\n",
    "    def fit(self, X,X_test, y,y_test):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "        '''\n",
    "        n,d = X_train.shape\n",
    "        lc = []\n",
    "        # Add bias to the feature space\n",
    "        X_std = np.c_[np.ones((n,1)), X_train]\n",
    "        # Intialize variables for regression\n",
    "        self.theta = np.matrix(np.random.randn((d + 1))).T\n",
    "        self.JHist = []\n",
    "        for i in range(self.maxNumIters):\n",
    "            theta_old = self.theta\n",
    "            gradient= self.computeGradient(self.theta,X_std,y_train,self.regLambda)\n",
    "            self.theta = self.theta - (self.alpha*gradient)\n",
    "#             if(self.hasConverged(self.theta,theta_old)==True):\n",
    "#                 break\n",
    "            if(i % 100 == 0):\n",
    "                y_pred = self.predict(X_test)\n",
    "                acc = accuracy_score(y_pred,y_test)\n",
    "                lc.append((i,acc))\n",
    "        return lc\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "        Returns:\n",
    "            an n-dimensional numpy vector of the predictions\n",
    "        '''\n",
    "        # Add bias to the feature space\n",
    "        X_std = np.c_[np.ones((X.shape[0],1)),X]\n",
    "        # Predict the labels\n",
    "        yhat = self.sigmoid(X_std.dot(self.theta))\n",
    "        for i in range(yhat.shape[0]):\n",
    "            if (yhat[i] > 0.5):\n",
    "                yhat[i] = 1\n",
    "            else:\n",
    "                yhat[i] = 0\n",
    "        yhat = np.asarray(yhat)\n",
    "        yhat = yhat.reshape((yhat.shape[0],1))\n",
    "        return yhat\n",
    "\n",
    "\n",
    "    def sigmoid(self, Z):\n",
    "    \t'''\n",
    "    \tComputes the sigmoid function 1/(1+exp(-z))\n",
    "    \t'''\n",
    "    \treturn 1.0/(1.0 + np.exp(-Z))\n",
    "\n",
    "    def hasConverged(self, theta_new, theta_old):\n",
    "        '''\n",
    "        Computes the change in the values of theta (normalized).\n",
    "        '''\n",
    "        x = theta_new - theta_old\n",
    "        if (np.linalg.norm(x, ord=self.regNorm) <= self.epsilon):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    TEMPLATE FOR MACHINE LEARNING HOMEWORK\n",
    "    AUTHOR Eric Eaton\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "class LogisticRegressionAdagrad:\n",
    "\n",
    "    def __init__(self, alpha = 0.0001, regLambda=0.01, regNorm=2, epsilon=0.0001, maxNumIters = 10):\n",
    "        '''\n",
    "        Constructor\n",
    "        Arguments:\n",
    "            alpha is the learning rate\n",
    "            regLambda is the regularization parameter\n",
    "            regNorm is the type of regularization (either L1 or L2, denoted by a 1 or a 2)\n",
    "            epsilon is the convergence parameter\n",
    "            maxNumIters is the maximum number of iterations to run\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.regLambda = regLambda\n",
    "        self.regNorm = regNorm\n",
    "        self.epsilon = epsilon\n",
    "        self.maxNumIters =  maxNumIters\n",
    "        self.JHist = None\n",
    "        self.theta = None\n",
    "        self.G = None\n",
    "        \n",
    "    def computeCost(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
    "        '''\n",
    "        yhat = self.sigmoid(X*theta)\n",
    "        if (self.regNorm == 2):\n",
    "            norm_term = (self.regLambda/2.0)* (theta.T*theta )\n",
    "        else:\n",
    "            norm_term = (self.regLambda/2.0)* (np.sum(theta))\n",
    "        J = -(y.T*np.log(yhat)) - ((1.0-y).T*(np.log(1.0 - yhat))) + norm_term  \n",
    "        J_scalar = J.item((0,0))\n",
    "        return J_scalar\n",
    "       \n",
    "    def computeGradient(self, theta, X, y, regLambda):\n",
    "        '''\n",
    "        Computes the gradient of the objective function\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "            regLambda is the scalar regularization constant\n",
    "        Returns:\n",
    "            the gradient, an d-dimensional vector\n",
    "        '''\n",
    "        yhat = self.sigmoid(X*theta)\n",
    "        gradient = (X.T* (yhat - y))\n",
    "        # Account for regularization in theta\n",
    "        if (self.regNorm == 2):\n",
    "            for k in range(1,len(theta)):\n",
    "                gradient[k]= gradient[k] + (regLambda*theta[k])\n",
    "        else:\n",
    "            for k in range(1,len(theta)):\n",
    "                gradient[k]= gradient[k] + (regLambda*np.sign(theta[k]))\n",
    "        gradient[0] = sum(yhat-y)\n",
    "        return gradient\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains the model\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "            y is an n-dimensional numpy vector\n",
    "        '''\n",
    "        n,d = X.shape\n",
    "        # Add bias to the feature space\n",
    "        X_std = np.c_[np.ones((n,1)), X]\n",
    "        # Intialize variables for regression\n",
    "        self.theta = np.matrix(np.random.randn((d + 1))).T\n",
    "        X_copy = X_std\n",
    "        X_copy = np.c_[X_std,y]\n",
    "        self.G = np.zeros((d+1,1))\n",
    "        for i in range(self.maxNumIters):\n",
    "            np.random.shuffle(X_copy)\n",
    "            X_sample = X_copy[:,0:d+1]\n",
    "            y_sample = X_copy[:,-1]\n",
    "            for k in range(n):   \n",
    "                theta_old = self.theta\n",
    "                X_new = X_sample[k].reshape(1,d+1)\n",
    "                gradient = self.computeGradient(self.theta,X_new,y_sample[k], self.regLambda)\n",
    "                # Adapt the alpha by the past values of gradient\n",
    "                self.G = self.G + np.square(gradient)\n",
    "                for k in range(0,len(gradient)):\n",
    "                     self.theta = self.theta - (self.alpha * gradient/(np.sqrt(self.G[k]) + 1e-4))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Used the model to predict values for each instance in X\n",
    "        Arguments:\n",
    "            X is a n-by-d numpy matrix\n",
    "        Returns:\n",
    "            an n-dimensional numpy vector of the predictions\n",
    "        '''\n",
    "        \n",
    "        # Add bias to the feature space\n",
    "        X_std = np.c_[np.ones((X.shape[0],1)),X]\n",
    "        # Predict the labels\n",
    "        yhat = self.sigmoid(X_std.dot(self.theta))\n",
    "        for i in range(yhat.shape[0]):\n",
    "            if (yhat[i] > 0.5):\n",
    "                yhat[i] = 1\n",
    "            else:\n",
    "                yhat[i] = 0\n",
    "        yhat = np.asarray(yhat)\n",
    "        yhat = yhat.reshape((yhat.shape[0],1))\n",
    "        return yhat\n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        '''\n",
    "        Computes the sigmoid function 1/(1+exp(-z))\n",
    "        '''\n",
    "        return 1.0/(1.0 + np.exp(-Z))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################\n",
    "\"WDBC Data\"\n",
    "wb_data= pd.read_csv(\"data/wdbc.dat\",sep = \",\",header = None)\n",
    "\n",
    "######################################################################################################\n",
    "df1, df2, df3,df4,df5 = np.split(wb_data.iloc[:-4,:],5)\n",
    "train_1 = pd.concat([df1,df2,df3,df4])\n",
    "test_1 = df5\n",
    "train_2 = pd.concat([df1,df2,df3,df5])\n",
    "test_2 = df4\n",
    "train_3 = pd.concat([df1,df2,df5,df4])\n",
    "test_3 = df3\n",
    "train_4 = pd.concat([df1,df5,df3,df4])\n",
    "test_4 = df2\n",
    "train_5 = pd.concat([df5,df2,df3,df4])\n",
    "test_5 = df1\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# X = wb_data.iloc[:,:-1]\n",
    "# X = np.array(X, dtype=\"float64\")\n",
    "# y = np.array(wb_data[30],dtype=object)\n",
    "# y = y.astype(str)\n",
    "# for j in range(y.shape[0]):\n",
    "#     if y[j] == 'M':\n",
    "#         y[j] = 1\n",
    "#     else:\n",
    "#         y[j] = 0\n",
    "# y = y.astype(int).reshape(y.shape[0],1)\n",
    "\n",
    "clf1= LogisticRegression(maxNumIters = 1500)\n",
    "clf2= LogisticRegression(maxNumIters = 2000)\n",
    "# r1 = clf1.fit(X,y)\n",
    "# x1= []\n",
    "# y1=[]\n",
    "# for i in range(len(r1)):\n",
    "#     x1.append(r1[i][0])\n",
    "#     y1.append(r1[i][1])\n",
    "# plt.plot(x1,y1)\n",
    "\n",
    "# y = y.astype(int).reshape(y.shape[0],1)\n",
    "# r2 = clf2.fit(X,y)\n",
    "# x2= []\n",
    "# y2=[]\n",
    "# for i in range(len(r2)):\n",
    "#     x2.append(r2[i][0])\n",
    "#     y2.append(r2[i][1])\n",
    "# plt.plot(x2,y2)\n",
    "# plt.show()\n",
    "acc1 = []\n",
    "acc2 = []\n",
    "# accTrain = []\n",
    "# #     for i in range(5):\n",
    "train = [train_1,train_2,train_3,train_4,train_5]\n",
    "test = [test_1,test_2,test_3,test_4,test_5]\n",
    "\n",
    "for i in range(5):\n",
    "    X_train = train[i].iloc[:,:-1]\n",
    "    X_train = np.array(X_train, dtype=\"float64\")\n",
    "    y_train = np.array(train[i][30],dtype=object)\n",
    "    y_train = y_train.astype(str)\n",
    "    for j in range(y_train.shape[0]):\n",
    "        if y_train[j] == 'M':\n",
    "            y_train[j] = 1\n",
    "        else:\n",
    "            y_train[j] = 0\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "    X_test = test[i].iloc[:,:-1]\n",
    "    X_test = np.array(X_test, dtype=\"float64\")\n",
    "    y_test = np.array(test[i][30],dtype=object)\n",
    "    y_test = y_test.astype(str)\n",
    "    for j in range(y_test.shape[0]):\n",
    "        if y_test[j] == 'M':\n",
    "            y_test[j] = 1\n",
    "        else:\n",
    "            y_test[j] = 0\n",
    "    y_test = y_test.astype(int)\n",
    "    # Standardize the data\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train- mean) / std\n",
    "    X_test = (X_test - mean)/std\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    acc1 = clf1.fit(X_train, X_test,y_train,y_test)\n",
    "    acc2 = clf2.fit(X_train, X_test,y_train,y_test)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1 = []\n",
    "y1 = []\n",
    "x2 = []\n",
    "y2 = []\n",
    "\n",
    "for i in range(len(acc1)):\n",
    "    x1.append(acc1[i][0])\n",
    "    y1.append(acc1[i][1])\n",
    "plt.plot(x1,y1,'red')\n",
    "for i in range(len(acc2)):\n",
    "    x2.append(acc2[i][0])\n",
    "    y2.append(acc2[i][1])\n",
    "plt.plot(x2,y2,'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6382608695652173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "\"Retinopathy Data\"\n",
    "r_data = pd.read_csv(\"data/retinopathy.dat\",sep = \",\",header = None)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "df1,df2,df3,df4,df5 = np.split(r_data.iloc[:-1,:],5)\n",
    "train_1 = pd.concat([df1,df2,df3,df4])\n",
    "test_1 = df5\n",
    "train_2 = pd.concat([df1,df2,df3,df5])\n",
    "test_2 = df4\n",
    "train_3 = pd.concat([df1,df2,df5,df4])\n",
    "test_3 = df3\n",
    "train_4 = pd.concat([df1,df5,df3,df4])\n",
    "test_4 = df2\n",
    "train_5 = pd.concat([df5,df2,df3,df4])\n",
    "test_5 = df1\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "C = 0.01  # value of C for the SVMs\n",
    "# create an instance of SVM with the custom kernel and train it\n",
    "clf = svm.SVC(C = C,gamma =0.1,kernel='linear')\n",
    "\n",
    "acc = []\n",
    "# #     for i in range(5):\n",
    "train = [train_1,train_2,train_3,train_4,train_5]\n",
    "test = [test_1,test_2,test_3,test_4,test_5]\n",
    "\n",
    "for i in range(5):\n",
    "    X_train = train[i].iloc[:,:-1]\n",
    "    X_train = np.array(X_train, dtype=\"float64\")\n",
    "    y_train = np.array(train[i][19],dtype=object)\n",
    "    y_train = y_train.astype(int)\n",
    "    X_test = test[i].iloc[:,:-1]\n",
    "    X_test = np.array(X_test, dtype=\"float64\")\n",
    "    y_test = np.array(test[i][19],dtype=object)\n",
    "    y_test = y_test.astype(int)\n",
    "    # Standardize the data\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train- mean) / std\n",
    "    X_test = (X_test - mean)/std\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    acc.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7725490196078432\n"
     ]
    }
   ],
   "source": [
    "######################################################################################################\n",
    "\"Diabetes Data\"\n",
    "d_data = pd.read_csv(\"data/diabetes.dat\",sep = \",\",header = None)\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "df1,df2,df3,df4,df5 = np.split(d_data.iloc[:-3,:],5)\n",
    "train_1 = pd.concat([df1,df2,df3,df4])\n",
    "test_1 = df5\n",
    "train_2 = pd.concat([df1,df2,df3,df5])\n",
    "test_2 = df4\n",
    "train_3 = pd.concat([df1,df2,df5,df4])\n",
    "test_3 = df3\n",
    "train_4 = pd.concat([df1,df5,df3,df4])\n",
    "test_4 = df2\n",
    "train_5 = pd.concat([df5,df2,df3,df4])\n",
    "test_5 = df1\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = LogisticRegression(regNorm=1)\n",
    "acc = []\n",
    "# #     for i in range(5):\n",
    "train = [train_1,train_2,train_3,train_4,train_5]\n",
    "test = [test_1,test_2,test_3,test_4,test_5]\n",
    "\n",
    "for i in range(5):\n",
    "    X_train = train[i].iloc[:,:-1]\n",
    "    X_train = np.array(X_train, dtype=\"float64\")\n",
    "    y_train = np.array(train[i][8],dtype=object)\n",
    "    y_train = y_train.astype(str)\n",
    "    for j in range(y_train.shape[0]):\n",
    "        if y_train[j] == 'tested_positive':\n",
    "            y_train[j] = 1\n",
    "        else:\n",
    "            y_train[j] = 0\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "    X_test = test[i].iloc[:,:-1]\n",
    "    X_test = np.array(X_test, dtype=\"float64\")\n",
    "    y_test = np.array(test[i][8],dtype=object)\n",
    "    y_test  = y_test.astype(str)\n",
    "    for j in range(y_test.shape[0]):\n",
    "        if y_test[j] == 'tested_positive':\n",
    "            y_test[j] = 1\n",
    "        else:\n",
    "            y_test[j] = 0\n",
    "    y_test = y_test.astype(int)\n",
    "    # Standardize the data\n",
    "    mean = X_train.mean(axis=0)\n",
    "    std = X_train.std(axis=0)\n",
    "    X_train = (X_train- mean) / std\n",
    "    X_test = (X_test - mean)/std\n",
    "    y_train = y_train.reshape(y_train.shape[0],1)\n",
    "    y_test = y_test.reshape(y_test.shape[0],1)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    acc.append(accuracy)\n",
    "print(np.mean(acc))              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "\"Learning Curve over different Models on retinopathy data\"\n",
    "###########################################################################################\n",
    "\n",
    "n = [100,1000,2000,5000,10000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
