{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import RandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms.functional import normalize\n",
    "#from google.colab import drive                # IF you are using COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(x_data_filepath, y_data_filepath):\n",
    "    X = np.load(x_data_filepath)\n",
    "    y = np.load(y_data_filepath)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Extracting and loading data\n",
    "############################################################\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.len = len(X)           \n",
    "        if torch.cuda.is_available():\n",
    "            self.x_data = torch.from_numpy(X).float().cuda()\n",
    "            self.y_data = torch.from_numpy(y).long().cuda()\n",
    "        else:\n",
    "            self.x_data = torch.from_numpy(X).float()\n",
    "            self.y_data = torch.from_numpy(y).long()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validation(x_train,y_train):\n",
    "    \"\"\"\n",
    "    Randomly choose 20 percent of the training data as validation data.\n",
    "\n",
    "    Args:\n",
    "        x_train: training images in shape (num_images,3,image_H,image_W)\n",
    "        y_train: training labels in shape (num_images,)\n",
    "    Returns:\n",
    "        new_x_train: training images in shape (0.8*num_images,3,image_H,image_W)\n",
    "        new_y_train: training labels in shape (0.8*num_images,)\n",
    "        x_val: validation images in shape (0.2*num_images,3,image_H,image_W)\n",
    "        y_val: validation labels in shape (0.2*num_images,)\n",
    "    \"\"\"\n",
    "    data = Dataset(x_train,y_train)\n",
    "    train_size = int(np.floor(0.8* len(data)))\n",
    "    indices= list(range(len(data)))\n",
    "    valid_size = len(data) - train_size\n",
    "    np.random.shuffle(indices)\n",
    "    train_mapping=indices[valid_size:]\n",
    "    valid_mapping=indices[:valid_size]\n",
    "    new_x_train = data.x_data[train_mapping]\n",
    "    x_val = data.x_data[valid_mapping]\n",
    "    new_y_train = data.y_data[train_mapping]\n",
    "    y_val = data.y_data[valid_mapping]\n",
    "    \n",
    "    return new_x_train,new_y_train,x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Normalize each input image\n",
    "\n",
    "    Args:\n",
    "        image: the input image in shape (3,image_H,image_W)\n",
    "    Returns:\n",
    "        norimg: the normalized image in the same shape as the input\n",
    "    \"\"\"\n",
    "    np_x = image.copy()\n",
    "    # Mean per channel\n",
    "    red_mean = np.mean(np_x[0,:,:])\n",
    "    green_mean = np.mean(np_x[1,:,:])\n",
    "    blue_mean = np.mean(np_x[2,:,:])\n",
    "    # Standard Deviation per channel\n",
    "    red_std = np.std(np_x[0,:,:])\n",
    "    green_std = np.std(np_x[1,:,:])\n",
    "    blue_std = np.std(np_x[2,:,:])\n",
    "    # normalize in tensor\n",
    "    np_x[0,:,:] = (np_x[0,:,:] - red_mean)/red_std\n",
    "    np_x[1,:,:] = (np_x[1,:,:] - green_mean)/green_std\n",
    "    np_x[2,:,:] = (np_x[2,:,:] - blue_mean)/blue_std\n",
    "    norimg = torch.tensor(np_x)\n",
    "    return norimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "# Optimized Neural Network\n",
    "############################################################\n",
    "class OptimizedNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OptimizedNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size = 3,stride=1, padding=0)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size = 3,stride=1, padding=0)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size = 3,stride=1, padding=0)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size = 2)\n",
    "        self.fc1 = nn.Linear(3072, 200)\n",
    "        self.dropout1 = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(200, 5)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Normalisation\n",
    "        x_numpy = x.numpy()\n",
    "        x = normalize_image(x_numpy)\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = self.pool1(out)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = self.pool2(out)\n",
    "        out = F.relu(self.conv3(out))\n",
    "        out = self.pool3(out)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_NN(neural_network, train_loader, validation_loader, loss_function, optimizer,num_epochs):\n",
    "    \"\"\"\n",
    "    Runs experiment on the model neural network given a train loader, loss function and optimizer and find validation \n",
    "    accuracy for each epoch given the validation_loader.\n",
    "\n",
    "    Args:\n",
    "        neural_network (NN model that extends torch.nn.Module): For example, it should take an instance of either\n",
    "                                                                FeedForwardNN or ConvolutionalNN,\n",
    "        train_loader (DataLoader),\n",
    "        validation_loader (DataLoader),\n",
    "        loss_function (torch.nn.CrossEntropyLoss),\n",
    "        optimizer (optim.SGD)\n",
    "        num_epochs (number of iterations)\n",
    "    Returns:\n",
    "        tuple: First position, training accuracies of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               Second position, training loss of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               third position, validation accuracy of each epoch formatted in an array of shape (num_epochs,1).\n",
    "               \n",
    "    \"\"\"\n",
    "    accuracy = np.zeros(shape=(num_epochs,1))\n",
    "    val_accuracy = np.zeros(shape=(num_epochs,1))\n",
    "    loss_np = np.zeros(shape=(num_epochs,1))\n",
    "    iterator = 0\n",
    "    for epoch in range(1, num_epochs+1): ## run the model for 10 epochs\n",
    "        train_loss = []\n",
    "        ## training part \n",
    "        neural_network.train()\n",
    "        correct = 0\n",
    "        acc = 0\n",
    "        val_acc = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            ## forward propagation\n",
    "            output = neural_network(data)\n",
    "            ## loss calculation\n",
    "            loss = loss_function(output, target)\n",
    "            ## backward propagation\n",
    "            loss.backward()\n",
    "            ## weight optimization\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "            ## accuracy computation for each batch\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        acc = correct.item()/len(train_loader.dataset)\n",
    "        ## evaluation part \n",
    "        neural_network.eval()\n",
    "        correct = 0\n",
    "        for data, target in validation_loader:\n",
    "            output = neural_network(data)\n",
    "            loss = loss_function(output, target)\n",
    "            ## accuracy computation on validation set\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        val_acc=correct.item()/len(validation_loader.dataset)   \n",
    "        # Add values for each epoch to the arrays\n",
    "        accuracy[iterator] = acc\n",
    "        val_accuracy[iterator] = val_acc\n",
    "        loss_np[iterator] = np.mean(train_loss)\n",
    "        iterator+=1\n",
    "    \n",
    "    return accuracy,loss_np,val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## Data Preprocessing #################################################\n",
    "\n",
    "images, labels = extract_data('data\\images_train.npy', 'data\\labels_train.npy')\n",
    "x_train, y_train, x_val, y_val = create_validation(images,labels)\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "validation_dataset = torch.utils.data.TensorDataset(x_val,y_val)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.40307102]\n",
      " [0.40882917]\n",
      " [0.46641075]\n",
      " [0.51055662]\n",
      " [0.51247601]\n",
      " [0.56621881]\n",
      " [0.59309021]\n",
      " [0.59692898]\n",
      " [0.62763916]\n",
      " [0.63723608]\n",
      " [0.61996161]\n",
      " [0.64491363]\n",
      " [0.64491363]\n",
      " [0.66026871]\n",
      " [0.68330134]\n",
      " [0.6890595 ]\n",
      " [0.69289827]\n",
      " [0.67946257]\n",
      " [0.70057582]\n",
      " [0.69289827]\n",
      " [0.7293666 ]\n",
      " [0.7293666 ]\n",
      " [0.75815739]\n",
      " [0.74664107]\n",
      " [0.73704415]\n",
      " [0.74664107]\n",
      " [0.73704415]\n",
      " [0.77159309]\n",
      " [0.77351248]\n",
      " [0.76583493]\n",
      " [0.76007678]\n",
      " [0.76583493]\n",
      " [0.75431862]\n",
      " [0.77927063]\n",
      " [0.77543186]\n",
      " [0.75431862]\n",
      " [0.75047985]\n",
      " [0.77159309]\n",
      " [0.76007678]\n",
      " [0.7696737 ]]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 40\n",
    "optimized_nn = OptimizedNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(optimized_nn.parameters(), lr=0.001)\n",
    "accuracy, loss_np, val_accuracy = train_val_NN(optimized_nn,train_loader, validation_loader, criterion, optimizer,num_epochs)\n",
    "\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################## training on complete training data #######################################\n",
    "\n",
    "def create_train(x_train,y_train):\n",
    "    data = Dataset(x_train,y_train)\n",
    "    train_size = int(np.floor(1.0* len(data)))\n",
    "    indices= list(range(len(data)))\n",
    "    valid_size = len(data) - train_size\n",
    "    np.random.shuffle(indices)\n",
    "    train_mapping=indices[valid_size:]\n",
    "    valid_mapping=indices[:valid_size]\n",
    "    new_x_train = data.x_data[train_mapping]\n",
    "    x_val = data.x_data[valid_mapping]\n",
    "    new_y_train = data.y_data[train_mapping]\n",
    "    y_val = data.y_data[valid_mapping]\n",
    "    \n",
    "    return new_x_train,new_y_train,x_val,y_val\n",
    "\n",
    "def train_NN(neural_network, train_loader, loss_function, optimizer,num_epochs):\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1): ## run the model for 10 epochs\n",
    "        train_loss = []\n",
    "        ## training part \n",
    "        neural_network.train()\n",
    "        correct = 0\n",
    "        acc = 0\n",
    "        val_acc = 0\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            ## forward propagation\n",
    "            output = neural_network(data)\n",
    "            ## loss calculation\n",
    "            loss = loss_function(output, target)\n",
    "            ## backward propagation\n",
    "            loss.backward()\n",
    "            ## weight optimization\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "images, labels = extract_data('data\\images_train.npy', 'data\\labels_train.npy')\n",
    "x_train, y_train, x_val, y_val = create_validation(images,labels)\n",
    "train_dataset = torch.utils.data.TensorDataset(x_train,y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32)\n",
    "\n",
    "train_NN(optimized_nn,train_loader,criterion, optimizer,num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_NN(neural_network, test_loader):\n",
    "  \n",
    "    \"\"\"\n",
    "    Runs experiment on the model neural network given a test loader, loss function and optimizer.\n",
    "\n",
    "    Args:\n",
    "        neural_network (NN model that extends torch.nn.Module): For example, it should take an instance of either\n",
    "                                                                FeedForwardNN or ConvolutionalNN,\n",
    "        test_loader (DataLoader), (make sure the loader is not shuffled)\n",
    "    Returns:\n",
    "        your predictions         \n",
    "    \"\"\"\n",
    "    neural_network.eval()\n",
    "    Preds = torch.LongTensor()\n",
    "    \n",
    "    for _, data in enumerate(test_loader):\n",
    "        data = data[0]\n",
    "        output = neural_network(data)\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        Preds = torch.cat((Preds, pred), dim=0)\n",
    "\n",
    "    return Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load('data\\images_test.npy')\n",
    "test_data = torch.tensor(test_data, dtype = torch.float32)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_data)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "Preds = test_NN(optimized_nn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('HW4_preds.txt', 'w', encoding = 'utf8') as file:\n",
    "    for i in range(len(test_loader.dataset)):\n",
    "        file.write(str(Preds[i].item())+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
